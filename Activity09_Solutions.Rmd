---
title: "Activity 9 - Regression in R"
author: "Dr. VanKrevelen -- STS3470 (Spring 2023)"
date: 'Updated: `r Sys.Date()`'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Directions:** Use this .RMD script to record your code / output / answers.

Remember that with a .RMD file, you can run individual code chunks by clicking the green triangle button in the top right of the chunk or by putting your cursor on a line and hitting Cmd+Return (Mac) or Ctrl+Return (PC) like you would in a .R script. When you use the Knit button, it will run all the code in order in a fresh R session and produce your knitted file.

Have questions? Find a classmate or Dr. V to ask!



# Part A - Simple linear regression

Simple linear regression is most often a way of describing a relationship between two quantitative variables. Before calculating a simple linear regression line, we might make a scatterplot and calculate a correlation coefficient, r. You can use the `cor()` function to find a correlation between two vectors.

1. Using the `mtcars` data frame, create a scatterplot of the weight (`wt`) of cars on the x-axis and the mileage (`mpg`) of cars on the y-axis. Do the two variables have an approximately linear relationship between them? Is the relationship positive or negative? (Bonus: Add a simple linear regression line to your graph)

```{r, message = FALSE,warning = FALSE, out.width = "50%"}
library(ggplot2)
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

**Answer:**  There is a roughly linear negative relationship between the variables.

<br>


2. Calculate the correlation between the weight (`wt`) of cars and the mileage (`mpg`). You will enter the two vectors separated by a comma using the form `mtcars$___` for each vector. Enter the correlation below.

```{r}
cor(mtcars$mpg, mtcars$wt)
```

**r =** - 0.87

<br>


3. We can use the `lm()` function to calculate the equation for a simple linear regression line. The general format is `lm(response ~ explanatory)`. You can either enter these as vectors (like you did with the `cor()` function) or enter just the variable name and add a `data = mtcars` argument. Write code to calculate the simple linear regression line. Store the output as an object called `out` and then print that object. Update $b_0$ and $b_1$ below to show your equation.

```{r}
out <- lm(mpg ~ wt, data = mtcars)
out
```

$\hat{y} = 37.285 - 5.344 * weight$

<br>


4. If we want more information about our linear regression, we can use the `summary()` function on our `lm()` output. This will give us standard errors, p-values, $R^2$ and more. Try this and fill in the answers below:

```{r}
summary(out)
```

**Slope p-value =** $1.29 * 10^{-10}$ or 0.000000000129

**Intercept p-value =** less than $2 * 10^{-16}$ or 0.0000000000000002

**R^2 =** 0.7528

<br>


5. Recall that the p-value associated with the slope corresponds to the hypothesis test: $H_0: \beta_1 = 0$ vs. $H_a: \beta_1 \ne 0$ where $\beta_1$ is the "true" slope if we had data for the entire population. In other words, the null hypothesis says there is no linear relationship between the weight of a car and its mileage. The alternative hypothesis says there IS some sort of linear relationship between the two. Write a conclusion for this test below that is in context of this example:

**Conclusion:** With such a small p-value for my slope, I reject the null, and I have very strong evidence there is a linear relationship between the weight of a car and the mileage of a car.

<br>


6. It's possible to run linear regressions with no y-intercept or with just a y-intercept (not that we want to do either in this case...). Which code do you think is which below? How can you tell?

```{r}
lm(mpg ~ 1, data = mtcars)
lm(mpg ~ wt - 1, data = mtcars)
```

**Answer:** The first one gives us a model with just an intercept (and no slope). The second gives us a model with just a slope (and no intercept). In the second case, our slope is positive because we are making ourselves have a y-intercept of 0 (and we can't have negative mpg).

<br>


7. Calculate the mean miles per gallon for this data frame. Where have you seen this number? Why does it make sense you'd get the same number here as you did there?

```{r}
mean(mtcars$mpg)
```

**Answer:** This is the intercept estimate in the model with no explanatory variables. If we aren't using any other variables, our best prediction for a car's `mpg` is the average.

<br>



# Part B - Multiple linear regression

If we want to use multiple variables to predict a quantitative response, we need multiple linear regression. To add explanatory variables to a model in R, we separate them with a + sign in the `lm()` function. If we want to include an interaction between terms, we write `var1 * var2` where we replace var1 and var2 with the variable names. 

8. Write code that uses weight, horsepower (`hp`), and displacement (`disp`) to predict mileage. Use the `summary()` function on your model output to see more information. Does it make sense to use all three of these variables to predict `mpg`? Why or why not?

```{r}
out_mlr <- lm(mpg ~ wt + hp + disp, data = mtcars)
summary(out_mlr)
```

**Answer:** The slope for `disp` has a very high p-value associated with it. This suggests we would see results like this quite often in a world where slope had no linear relationship with `mpg`. For that reason, it doesn't make sense for us to use `disp` to help us predict `mpg`, so we should remove it from our model.

<br>


9. Re-run your model without the displacement variable (if you're not sure whey we are doing this, just ask!). Replace $b_0$, $b_1$, and $b_2$ below with the correct values for your model. Then write an interpretation of your y-intercept estimate and at least one of your slope estimates.

```{r}
out2_mlr <- lm(mpg ~ wt + hp, data = mtcars)
summary(out2_mlr)
```

$\hat{y} = 37.23 - 3.87783 * wt - 0.03177 * hp$

**Interpretation of $b_0$:** The estimated mpg (or population mean mpg) is 37.23 when the weight and horsepower of the car are 0. However, it's not possible to have a 0 pound and 0 horsepower car, so this is an extrapolation.

**Interpretation of $b_1$ and/or $b_2$:** 

- For every 1,000 pounds a car weighs, the predicted miles per gallon decreases by 3.88 (assuming horsepower stays constant).
- For everyone additional horsepower, the predicted miles per gallon decreases by 0.03 (assuming weight stays constant)

<br>


10. Below are some further questions to consider:

a. Did you get the same slope estimate for the weight variable in your model using simple linear regression and multiple linear regression? Why or why not?

**Answer:** No. This is because our simple linear regression didn't account for horsepower. It's possible heavier cars also tend to have more horsepower, and the simple linear regression model wouldn't be able to disentangle the impact of the two variables on miles per gallon.

<br>


b. The weight variable has a bigger estimate for the slope than horsepower does. Should we use that to determine that weight is "more important"? Why or why not?

**Answer:** Not necessarily. The slope estimate will be impacted by the units we use for our variable. For example, if we converted `wt` from being measured in 1,000s of pounds to being measured in pounds, our estimate for the slope would not be -0.0039, which would be smaller than the slope estimate for horsepower. (Try this out if you want!)

<br>


c. The $R^2$ value was essentially the same when we included or did not include displacement in our model. Can this tell us these two models are equally good? Why or why not?

**Answer:** No! $R^2$ will never decrease when you add explnatory variables to a model because if the variable isn't helpful for prediction, it will just be ignored in the formula. When comparing models with different numbers of explanatory variables, you need to use something other than just $R^2$ to decide which model is "better".

<br>


d. How does the correlation we got in part A relate to the $R^2$ value for our model that only used wt to predict mpg?

**Answer:** If we take r = -0.8677 and square it, we get $R^2$ = 0.75

<br>


# (Optional) Part C - Recreating other tests with regression

11. Many of the statistical tests we do could really be considered variations of regression. To illustrate this, consider our t-tests we did in activity 8. What p-value did we get with each test?

```{r, message = FALSE}
library(tidyr)
library(dplyr)

mysleep <- pivot_wider(sleep, id_cols = ID, names_from = group, 
                       names_prefix = "extra", values_from = extra)
mysleep <- mutate(mysleep, diff = extra1 - extra2)

t.test(mysleep$diff)

summary(lm(diff ~ 1, data = mysleep))
```

**Answer:** 0.00283

<br>


12. The last question shows us that a one-sample t-test with a two-sided alternative is the same as a simple linear regression with only an intercept (no explanatory variable). A simple linear regression with a **categorical** explanatory variable that has two levels can produce the same p-values as a two-sample t-test. Which test does it replicate: Welch's or Student's?

```{r}
hedgehogs <- read.csv("https://raw.githubusercontent.com/vank-stats/STS3470-Spring2023/main/hw3%20-%20hedgehogs.csv")
hedgehogs_sexknown <- filter(hedgehogs, Sex != "Unknown")
t.test(Age ~ Sex, data = hedgehogs_sexknown, var.equal = FALSE)
t.test(Age ~ Sex, data = hedgehogs_sexknown, var.equal = TRUE)

summary(lm(Age ~ Sex, data = hedgehogs_sexknown))
```

**Answer:** The p-value for the slope is the same as the p-value for Student's two-independent samples t-test (which assumes equal variances). 

<br>



13. Explore whether you could get a regression model to recreate an ANOVA test. Are there other tests you've learned that you're curious about replicating with regression?

Here is our one-way ANOVA from Notes 9 recreated with a simple linear regression. The p-value is so small that it's hard to tell that they are the same from the summary tables. However, notice that both tables list an F statistic of 34.7 in an F distribution with 5 and 66 degrees of freedom (see bottom of regression summary).

```{r}
out_aov <- aov(count ~ spray, data = InsectSprays)
summary(out_aov)

out_lm <- lm(count ~ spray, data = InsectSprays)
summary(out_lm)
```

<br>
<br>
<br>

The ANOVA output doesn't provide an overall p-value for a Two-way ANOVA, but the results will match a multiple linear regression result in this case as well. Here is the example from Notes 9.

```{r}
out_aov2 <- aov(len ~ supp * factor(dose), data = ToothGrowth)
summary(out_aov2)

out_lm2 <- lm(len ~ supp * factor(dose), data = ToothGrowth)
summary(out_lm2)
```

